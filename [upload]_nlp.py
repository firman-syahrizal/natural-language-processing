# -*- coding: utf-8 -*-
"""[UPLOAD] NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZWM7MxdyGAIDROmkxwH4lT7WMvFDq9GE

# **Goal**

**NOTE** This notebook is *slightly different* with the actual code.

This code is assigmnent to complete IDcamp 2023 in intermediete stage for machine learning path. The goal is to classify text into 3 classes, positive, neutral, and negative review. The dataset used about Amazon Reviews Datasets from kaggle.

# **Profile**
www.dicoding.com/users/firmansyahrizal/

# **Hubungkan dengan Google Drive**



1.   Mount your Google Drive
2.   Create a folder in Google Drive (untuk menyimpan pada folder tertentu)
"""

# Mount Google Drive

from google.colab import drive
drive.mount('/content/gdrive')

"""# **Persiapkan Dataframe**



1.   Transform dataets into **dataframe**
2.   Because the data is categorical, first **one-hot-encoding** the data


"""

# transform dataset menjadi dataframe

import pandas as pd
df = pd.read_csv('/content/gdrive/MyDrive/Colab Files/Dicoding/00-NLP/cleaned_reviews AMZ.csv')

# Lihat bentuk data
df.info()

# know our dataset!
df

# we just use 2 columns "sentiments" and "cleaned_review" column, so drop the other column
df = df.drop(columns=['cleaned_review_length', 'review_score'])
df

# one-hot-encoding

category = pd.get_dummies(df.sentiments)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='sentiments')
df_baru

# how many unique words, will be usefull for  and maxlen

# count unique words
unique_word_count = len(df['cleaned_review'].unique())
print(f"Number of unique words in data column: {unique_word_count}")
# digunakan untuk menentukan num_word

# hitung rata-rata kata disetiap row
# Apply str.split to each row to get a list of words
df['word_count'] = df['cleaned_review'].str.split().str.len()

# Hitung rata-rata banyak kata tiap baris
average_word_count = df['word_count'].mean()
print(f"Average number of words per row: {average_word_count}")
# digunakan untuk menetukan maxlen

# transform the dataframe into numpy array

pesan = df_baru['cleaned_review'].values
label = df_baru[['negative', 'positive', 'neutral']].values

# split the train and test dataset

from sklearn.model_selection import train_test_split
pesan_latih, pesan_test, label_latih, label_test = train_test_split(pesan, label, test_size=0.2)

# ubah data jadi float, baru ada untuk data AMAZON

pesan_latih = [str(item) for item in pesan_latih]
pesan_test = [str(item) for item in pesan_test]

# buat sequences

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# num_words 5000 - 50000, remember that the datasets just only 13.194 unique words
tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(pesan_latih)
#tokenizer.fit_on_texts(pesan_test) .ft hanya dilakukan pada train datasets saja

max_seq=55 # rata-rata panjang kalimat, meskipun rata-rata jumlah kata 30, tetapi cari nilai yang lebih merepresentasikan dataset
sekuens_latih = tokenizer.texts_to_sequences(pesan_latih)
sekuens_test = tokenizer.texts_to_sequences(pesan_test)

padded_latih = pad_sequences(sekuens_latih,
                             padding='post',
                             maxlen=max_seq,
                             truncating='post')
padded_test = pad_sequences(sekuens_test,
                            padding='post',
                            maxlen=max_seq,
                            truncating='post')

"""# **Build and Train the Model**

Dikarenakan untuk tujuan referensi dikemudian hari, beberapa layer/ perintah hanya dimatikan menggunakan #
"""

import tensorflow as tf
from tensorflow.keras.layers import LSTM

# menggunakan callback
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('/best_model.h5', save_weights_only=True, save_freq='epoch')
learning_rate_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)

jml_kata = len(tokenizer.word_index) + 1
dimensi_embedding = 256 # mulai 128, 256, ...
# buat model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=jml_kata, output_dim=dimensi_embedding, input_length=max_seq),
    tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Bidirectional(LSTM(16, return_sequences=True, dropout=0.3)),
    #tf.keras.layers.LSTM(16, return_sequences=True, dropout=0.5),
    #tf.keras.layers.LSTM(16, dropout=0.3),
    #tf.keras.layers.Flatten(),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),
    #tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')])

#compile model
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

num_epochs = 55
catatan = model.fit(padded_latih, label_latih,
                    epochs=num_epochs,
                    batch_size=32,
                    validation_data=(padded_test, label_test),
                    callbacks=[model_checkpoint, learning_rate_scheduler],
                    verbose=2)

# plot akurasi

import matplotlib.pyplot as plt
plt.plot(catatan.history['accuracy'])
plt.plot(catatan.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

# plot loss

plt.plot(catatan.history['loss'])
plt.plot(catatan.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()